from domain import *
from mdp_planning import\
    policy_model, value_model
from belief_update import\
    belief_update

import torch
import torch.tensor as tensor
import pyro
import pyro.distributions as dist
from pyro.contrib.autoname import scope
from pyro import poutine
import sys
import matplotlib.pyplot as plt
import numpy as np
from utils import Infer


def belief_policy_model(belief, t, discount=1.0, discount_factor=0.95, max_depth=10):
    state = states[pyro.sample("s%d" % t, belief)]
    return policy_model(state, t, discount=discount,
                        discount_factor=discount_factor,
                        max_depth=max_depth)

def belief_policy_model_guide(belief, t, discount=1.0, discount_factor=0.95, max_depth=10):
    # prior weights is uniform
    weights = pyro.param("action_weights", tensor([0.1, 0.1, 0.1]),
                         constraint=dist.constraints.simplex)
    state = states[pyro.sample("s%d" % t, belief)]
    # This is just to generate the other variables
    with poutine.block(hide=["a%d" % t]):
        # Need to hide 'at' generated by the policy_model;
        # we don't care about it; because that's what we are inferring.
        policy_model(state, t, discount=discount,
                     discount_factor=discount_factor,
                     max_depth=max_depth)
    # We eventually generate actions based on the weights
    action = pyro.sample("a%d" % t, dist.Categorical(weights))


def plan(belief, max_depth=3, discount_factor=0.95, lr=0.1, nsteps=100, print_losses=True):
    """nsteps (int) number of iterations to reduce the loss"""
    pyro.clear_param_store()
    svi = pyro.infer.SVI(belief_policy_model,
                         belief_policy_model_guide,
                         pyro.optim.Adam({"lr": lr}),
                         loss=pyro.infer.Trace_ELBO(retain_graph=True))
    Infer(svi, belief, 0,
          discount=1.0, discount_factor=discount_factor, max_depth=max_depth,
          num_steps=nsteps, print_losses=print_losses)
    return pyro.param("action_weights")


def simulate(state, sim_steps=10):
    """sim_steps (int) number of steps to run the POMDP"""
    # Simulate agent and planning and belief updates
    max_depth = 3
    discount_factor = 0.95        

    # prior belief
    belief = dist.Categorical(tensor([1., 1., 0.]))

    for i in range(sim_steps):
        print("\n--- Step %d ---" % i)
        print("State: %s" % state)
        print("Belief: %s" % belief.probs)
        weights = plan(belief, max_depth=max_depth,
                       discount_factor=discount_factor,
                       nsteps=50, print_losses=False)
        action = actions[torch.argmax(weights).item()]
        print("Action to take: %s" % action)
        print("Action weights: %s" % str(pyro.param("action_weights")))

        # state transition, observation, reward, belief update
        next_state = states[pyro.sample("s'", transition_dist(state, action))]
        observation = observations[pyro.sample("o", observation_dist(next_state, action))]
        reward = pyro.sample("r", reward_dist(state, action, next_state))
        print("Next State: %s" % next_state)
        print("Observation: %s" % observation)
        print("Reward: %s" % reward.item())

        print("Updating belief...")
        belief = belief_update(belief, action, observation, num_steps=1000,
                               print_losses=False)

        if next_state == "terminal":
            print("Done.")
            break

        # update state
        state = next_state        
    

if __name__ == "__main__":
    simulate("tiger-left")
